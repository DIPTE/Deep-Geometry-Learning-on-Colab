{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GBNet_colab.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNYKBmp5o+SSfAi4LjiN377",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPTE/Deep-Geometry-Learning-on-Colab/blob/master/GBNet_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndm_-V0Fhw6v",
        "outputId": "bcf72470-8558-4960-ae30-f3b6dcfc1f5c"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data\t gbnet.png  main.py   pretrained  util.py\n",
            "data.py  LICENSE    model.py  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oXUJlUth1Dx"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6CDzt-Ih5ha"
      },
      "source": [
        "!nvcc -V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pthh_d44h94t"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive')#切换工作目录"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWB2fF8xiBaZ",
        "outputId": "6e145643-08f5-4a3a-8182-05b2b7202088"
      },
      "source": [
        "!git clone https://github.com/ShiQiu0419/GBNet.git"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GBNet'...\n",
            "remote: Enumerating objects: 510, done.\u001b[K\n",
            "remote: Counting objects: 100% (163/163), done.\u001b[K\n",
            "remote: Compressing objects: 100% (151/151), done.\u001b[K\n",
            "remote: Total 510 (delta 90), reused 0 (delta 0), pack-reused 347\u001b[K\n",
            "Receiving objects: 100% (510/510), 906.68 KiB | 6.08 MiB/s, done.\n",
            "Resolving deltas: 100% (289/289), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rks0NvqfiE-u"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/GBNet')#切换工作目录"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr4VilZritCJ",
        "outputId": "fac1e14c-dae6-476a-94d5-76f47ee61390"
      },
      "source": [
        "!python main.py --exp_name=gbnet_modelnet40 --model=gbnet --dataset=modelnet40"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=16, dataset='modelnet40', dropout=0.5, emb_dims=1024, epochs=300, eval=False, exp_name='gbnet_modelnet40', k=20, lr=0.001, model='gbnet', model_path='', momentum=0.9, no_cuda=False, num_points=1024, seed=1, test_batch_size=8, use_sgd=True)\n",
            "Using GPU : 0 from 1 devices\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "GBNet(\n",
            "  (abem1): ABEM_Module(\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv1): Sequential(\n",
            "      (0): Conv2d(28, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (bn2): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Sequential(\n",
            "      (0): Conv2d(64, 14, kernel_size=(1, 20), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(14, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv3): Sequential(\n",
            "      (0): Conv2d(28, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (caa1): CAA_Module(\n",
            "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (query_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (key_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (value_conv): Sequential(\n",
            "        (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "    (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv4): Sequential(\n",
            "      (0): Conv2d(28, 64, kernel_size=(1, 20), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (caa2): CAA_Module(\n",
            "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (query_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (key_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (value_conv): Sequential(\n",
            "        (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "  )\n",
            "  (abem2): ABEM_Module(\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv1): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Sequential(\n",
            "      (0): Conv2d(64, 64, kernel_size=(1, 20), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv3): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (caa1): CAA_Module(\n",
            "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (query_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (key_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (value_conv): Sequential(\n",
            "        (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "    (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv4): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(1, 20), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (caa2): CAA_Module(\n",
            "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (query_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (key_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (value_conv): Sequential(\n",
            "        (0): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "  )\n",
            "  (abem3): ABEM_Module(\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv1): Sequential(\n",
            "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Sequential(\n",
            "      (0): Conv2d(128, 64, kernel_size=(1, 20), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv3): Sequential(\n",
            "      (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (caa1): CAA_Module(\n",
            "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (query_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (key_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (value_conv): Sequential(\n",
            "        (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "    (bn4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv4): Sequential(\n",
            "      (0): Conv2d(128, 128, kernel_size=(1, 20), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (caa2): CAA_Module(\n",
            "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (query_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (key_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (value_conv): Sequential(\n",
            "        (0): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "  )\n",
            "  (abem4): ABEM_Module(\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv1): Sequential(\n",
            "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv2): Sequential(\n",
            "      (0): Conv2d(256, 128, kernel_size=(1, 20), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv3): Sequential(\n",
            "      (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (caa1): CAA_Module(\n",
            "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (query_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (key_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (value_conv): Sequential(\n",
            "        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "    (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (conv4): Sequential(\n",
            "      (0): Conv2d(256, 256, kernel_size=(1, 20), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (caa2): CAA_Module(\n",
            "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bn3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (query_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (key_conv): Sequential(\n",
            "        (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (value_conv): Sequential(\n",
            "        (0): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
            "        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): ReLU()\n",
            "      )\n",
            "      (softmax): Softmax(dim=-1)\n",
            "    )\n",
            "  )\n",
            "  (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv): Sequential(\n",
            "    (0): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
            "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (caa): CAA_Module(\n",
            "    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (query_conv): Sequential(\n",
            "      (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (key_conv): Sequential(\n",
            "      (0): Conv1d(1024, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
            "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (value_conv): Sequential(\n",
            "      (0): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
            "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): ReLU()\n",
            "    )\n",
            "    (softmax): Softmax(dim=-1)\n",
            "  )\n",
            "  (linear1): Linear(in_features=2048, out_features=512, bias=False)\n",
            "  (bn_linear1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dp1): Dropout(p=0.5, inplace=False)\n",
            "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (bn_linear2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dp2): Dropout(p=0.5, inplace=False)\n",
            "  (linear3): Linear(in_features=256, out_features=40, bias=True)\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Use SGD\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "Train 0, loss: 3.413386, train acc: 0.182825, train avg acc: 0.078436\n",
            "Test 0, loss: 2.932748, test acc: 0.311994, test avg acc: 0.199703\n",
            "Current Best: 0.311994\n",
            "Train 1, loss: 2.764883, train acc: 0.406098, train avg acc: 0.221159\n",
            "Test 1, loss: 2.452361, test acc: 0.502026, test avg acc: 0.337605\n",
            "Current Best: 0.502026\n",
            "Train 2, loss: 2.503907, train acc: 0.507622, train avg acc: 0.315264\n",
            "Test 2, loss: 2.258494, test acc: 0.594003, test avg acc: 0.450808\n",
            "Current Best: 0.594003\n",
            "Train 3, loss: 2.356176, train acc: 0.580285, train avg acc: 0.393342\n",
            "Test 3, loss: 2.174992, test acc: 0.621556, test avg acc: 0.471948\n",
            "Current Best: 0.621556\n",
            "Train 4, loss: 2.267323, train acc: 0.611484, train avg acc: 0.436177\n",
            "Test 4, loss: 2.057015, test acc: 0.675041, test avg acc: 0.566640\n",
            "Current Best: 0.675041\n",
            "Train 5, loss: 2.178280, train acc: 0.646646, train avg acc: 0.477501\n",
            "Test 5, loss: 1.997633, test acc: 0.720016, test avg acc: 0.598017\n",
            "Current Best: 0.720016\n",
            "Train 6, loss: 2.122394, train acc: 0.672764, train avg acc: 0.510510\n",
            "Test 6, loss: 1.958942, test acc: 0.757293, test avg acc: 0.670616\n",
            "Current Best: 0.757293\n",
            "Train 7, loss: 2.072175, train acc: 0.696037, train avg acc: 0.539509\n",
            "Test 7, loss: 1.854882, test acc: 0.791734, test avg acc: 0.662884\n",
            "Current Best: 0.791734\n",
            "Train 8, loss: 2.017335, train acc: 0.718496, train avg acc: 0.572525\n",
            "Test 8, loss: 1.823442, test acc: 0.799028, test avg acc: 0.704622\n",
            "Current Best: 0.799028\n",
            "Train 9, loss: 1.979884, train acc: 0.734959, train avg acc: 0.600574\n",
            "Test 9, loss: 1.831133, test acc: 0.814425, test avg acc: 0.721895\n",
            "Current Best: 0.814425\n",
            "Train 10, loss: 1.937857, train acc: 0.755691, train avg acc: 0.626574\n",
            "Test 10, loss: 1.757857, test acc: 0.820097, test avg acc: 0.721971\n",
            "Current Best: 0.820097\n",
            "Train 11, loss: 1.916753, train acc: 0.767073, train avg acc: 0.650105\n",
            "Test 11, loss: 1.745215, test acc: 0.824149, test avg acc: 0.733215\n",
            "Current Best: 0.824149\n",
            "Train 12, loss: 1.893923, train acc: 0.776220, train avg acc: 0.657118\n",
            "Test 12, loss: 1.731292, test acc: 0.826175, test avg acc: 0.741698\n",
            "Current Best: 0.826175\n",
            "Train 13, loss: 1.863656, train acc: 0.786789, train avg acc: 0.673309\n",
            "Test 13, loss: 1.724259, test acc: 0.841572, test avg acc: 0.742174\n",
            "Current Best: 0.841572\n",
            "Train 14, loss: 1.853008, train acc: 0.794411, train avg acc: 0.683789\n",
            "Test 14, loss: 1.727891, test acc: 0.839141, test avg acc: 0.775424\n",
            "Train 15, loss: 1.838954, train acc: 0.794919, train avg acc: 0.691017\n",
            "Test 15, loss: 1.685374, test acc: 0.838331, test avg acc: 0.779988\n",
            "Train 16, loss: 1.813075, train acc: 0.810467, train avg acc: 0.710998\n",
            "Test 16, loss: 1.674841, test acc: 0.855754, test avg acc: 0.781959\n",
            "Current Best: 0.855754\n",
            "Train 17, loss: 1.807557, train acc: 0.813008, train avg acc: 0.716060\n",
            "Test 17, loss: 1.691842, test acc: 0.855754, test avg acc: 0.782477\n",
            "Current Best: 0.855754\n",
            "Train 18, loss: 1.802367, train acc: 0.810976, train avg acc: 0.711533\n",
            "Test 18, loss: 1.694035, test acc: 0.856564, test avg acc: 0.775285\n",
            "Current Best: 0.856564\n",
            "Train 19, loss: 1.788890, train acc: 0.822053, train avg acc: 0.730977\n",
            "Test 19, loss: 1.666976, test acc: 0.855754, test avg acc: 0.798029\n",
            "Train 20, loss: 1.770081, train acc: 0.829776, train avg acc: 0.738647\n",
            "Test 20, loss: 1.670793, test acc: 0.854943, test avg acc: 0.779465\n",
            "Train 21, loss: 1.773009, train acc: 0.827541, train avg acc: 0.740573\n",
            "Test 21, loss: 1.650074, test acc: 0.865478, test avg acc: 0.776221\n",
            "Current Best: 0.865478\n",
            "Train 22, loss: 1.768950, train acc: 0.830589, train avg acc: 0.744149\n",
            "Test 22, loss: 1.759401, test acc: 0.807536, test avg acc: 0.735628\n",
            "Train 23, loss: 1.772807, train acc: 0.826423, train avg acc: 0.735256\n",
            "Test 23, loss: 1.712650, test acc: 0.833063, test avg acc: 0.791738\n",
            "Train 24, loss: 1.764707, train acc: 0.828760, train avg acc: 0.741053\n",
            "Test 24, loss: 1.695275, test acc: 0.844003, test avg acc: 0.789942\n",
            "Train 25, loss: 1.747147, train acc: 0.839939, train avg acc: 0.753638\n",
            "Test 25, loss: 1.656966, test acc: 0.860616, test avg acc: 0.799564\n",
            "Train 26, loss: 1.745325, train acc: 0.842683, train avg acc: 0.763801\n",
            "Test 26, loss: 1.638489, test acc: 0.869125, test avg acc: 0.790419\n",
            "Current Best: 0.869125\n",
            "Train 27, loss: 1.737664, train acc: 0.844004, train avg acc: 0.760686\n",
            "Test 27, loss: 1.624544, test acc: 0.863857, test avg acc: 0.805116\n",
            "Train 28, loss: 1.735004, train acc: 0.845224, train avg acc: 0.765047\n",
            "Test 28, loss: 1.641399, test acc: 0.863857, test avg acc: 0.804971\n",
            "Train 29, loss: 1.736304, train acc: 0.841057, train avg acc: 0.758352\n",
            "Test 29, loss: 1.631280, test acc: 0.878849, test avg acc: 0.821657\n",
            "Current Best: 0.878849\n",
            "Train 30, loss: 1.728803, train acc: 0.846341, train avg acc: 0.766931\n",
            "Test 30, loss: 1.669479, test acc: 0.862642, test avg acc: 0.803302\n",
            "Train 31, loss: 1.728039, train acc: 0.844817, train avg acc: 0.760082\n",
            "Test 31, loss: 1.629327, test acc: 0.877634, test avg acc: 0.821593\n",
            "Train 32, loss: 1.728004, train acc: 0.842886, train avg acc: 0.764286\n",
            "Test 32, loss: 1.665903, test acc: 0.853728, test avg acc: 0.749942\n",
            "Train 33, loss: 1.730497, train acc: 0.842886, train avg acc: 0.760449\n",
            "Test 33, loss: 1.626339, test acc: 0.862642, test avg acc: 0.786767\n",
            "Train 34, loss: 1.715868, train acc: 0.851423, train avg acc: 0.772857\n",
            "Test 34, loss: 1.656827, test acc: 0.861831, test avg acc: 0.806378\n",
            "Train 35, loss: 1.718924, train acc: 0.850203, train avg acc: 0.771936\n",
            "Test 35, loss: 1.642610, test acc: 0.861021, test avg acc: 0.802791\n",
            "Train 36, loss: 1.709600, train acc: 0.855285, train avg acc: 0.782586\n",
            "Test 36, loss: 1.601900, test acc: 0.886953, test avg acc: 0.814680\n",
            "Current Best: 0.886953\n",
            "Train 37, loss: 1.707378, train acc: 0.860467, train avg acc: 0.788504\n",
            "Test 37, loss: 1.588943, test acc: 0.881280, test avg acc: 0.821110\n",
            "Train 38, loss: 1.703345, train acc: 0.857520, train avg acc: 0.782609\n",
            "Test 38, loss: 1.615471, test acc: 0.888979, test avg acc: 0.828802\n",
            "Current Best: 0.888979\n",
            "Train 39, loss: 1.708158, train acc: 0.855386, train avg acc: 0.780125\n",
            "Test 39, loss: 1.604935, test acc: 0.890194, test avg acc: 0.822384\n",
            "Current Best: 0.890194\n",
            "Train 40, loss: 1.697949, train acc: 0.861280, train avg acc: 0.787570\n",
            "Test 40, loss: 1.595753, test acc: 0.892626, test avg acc: 0.823890\n",
            "Current Best: 0.892626\n",
            "Train 41, loss: 1.701775, train acc: 0.853557, train avg acc: 0.777441\n",
            "Test 41, loss: 1.627753, test acc: 0.860211, test avg acc: 0.806785\n",
            "Train 42, loss: 1.694459, train acc: 0.858028, train avg acc: 0.784295\n",
            "Test 42, loss: 1.627318, test acc: 0.873582, test avg acc: 0.805477\n",
            "Train 43, loss: 1.687883, train acc: 0.864939, train avg acc: 0.793492\n",
            "Test 43, loss: 1.590851, test acc: 0.891815, test avg acc: 0.823174\n",
            "Train 44, loss: 1.683901, train acc: 0.865854, train avg acc: 0.793161\n",
            "Test 44, loss: 1.570135, test acc: 0.897488, test avg acc: 0.862674\n",
            "Current Best: 0.897488\n",
            "Train 45, loss: 1.688044, train acc: 0.865549, train avg acc: 0.792031\n",
            "Test 45, loss: 1.645818, test acc: 0.852107, test avg acc: 0.808401\n",
            "Train 46, loss: 1.671830, train acc: 0.873984, train avg acc: 0.809971\n",
            "Test 46, loss: 1.620658, test acc: 0.865073, test avg acc: 0.810942\n",
            "Train 47, loss: 1.678971, train acc: 0.863821, train avg acc: 0.793495\n",
            "Test 47, loss: 1.635008, test acc: 0.869125, test avg acc: 0.827866\n",
            "Train 48, loss: 1.673750, train acc: 0.867785, train avg acc: 0.799165\n",
            "Test 48, loss: 1.606137, test acc: 0.876418, test avg acc: 0.799134\n",
            "Train 49, loss: 1.684067, train acc: 0.859451, train avg acc: 0.788224\n",
            "Test 49, loss: 1.636731, test acc: 0.866288, test avg acc: 0.816942\n",
            "Train 50, loss: 1.670742, train acc: 0.868394, train avg acc: 0.799404\n",
            "Test 50, loss: 1.580077, test acc: 0.887763, test avg acc: 0.819570\n",
            "Train 51, loss: 1.662307, train acc: 0.873476, train avg acc: 0.811651\n",
            "Test 51, loss: 1.638899, test acc: 0.877634, test avg acc: 0.832157\n",
            "Train 52, loss: 1.675900, train acc: 0.866463, train avg acc: 0.795371\n",
            "Test 52, loss: 1.573618, test acc: 0.897893, test avg acc: 0.838744\n",
            "Current Best: 0.897893\n",
            "Train 53, loss: 1.678959, train acc: 0.867581, train avg acc: 0.802022\n",
            "Test 53, loss: 1.641907, test acc: 0.867099, test avg acc: 0.813273\n",
            "Train 54, loss: 1.671237, train acc: 0.870732, train avg acc: 0.800686\n",
            "Test 54, loss: 1.615620, test acc: 0.880470, test avg acc: 0.818651\n",
            "Train 55, loss: 1.671831, train acc: 0.867988, train avg acc: 0.801337\n",
            "Test 55, loss: 1.606153, test acc: 0.884522, test avg acc: 0.835558\n",
            "Train 56, loss: 1.659846, train acc: 0.875813, train avg acc: 0.812633\n",
            "Test 56, loss: 1.645854, test acc: 0.873582, test avg acc: 0.827512\n",
            "Train 57, loss: 1.665210, train acc: 0.873476, train avg acc: 0.806645\n",
            "Test 57, loss: 1.583486, test acc: 0.891815, test avg acc: 0.850250\n",
            "Train 58, loss: 1.652744, train acc: 0.876829, train avg acc: 0.813648\n",
            "Test 58, loss: 1.602135, test acc: 0.886548, test avg acc: 0.821390\n",
            "Train 59, loss: 1.656675, train acc: 0.877033, train avg acc: 0.811182\n",
            "Test 59, loss: 1.602459, test acc: 0.875203, test avg acc: 0.819750\n",
            "Train 60, loss: 1.654030, train acc: 0.879675, train avg acc: 0.817054\n",
            "Test 60, loss: 1.565999, test acc: 0.895462, test avg acc: 0.826547\n",
            "Train 61, loss: 1.654834, train acc: 0.872561, train avg acc: 0.808080\n",
            "Test 61, loss: 1.617268, test acc: 0.870746, test avg acc: 0.841634\n",
            "Train 62, loss: 1.652879, train acc: 0.877947, train avg acc: 0.817116\n",
            "Test 62, loss: 1.582329, test acc: 0.887358, test avg acc: 0.830855\n",
            "Train 63, loss: 1.646673, train acc: 0.880488, train avg acc: 0.819345\n",
            "Test 63, loss: 1.570031, test acc: 0.899109, test avg acc: 0.834669\n",
            "Current Best: 0.899109\n",
            "Train 64, loss: 1.651907, train acc: 0.875407, train avg acc: 0.813728\n",
            "Test 64, loss: 1.555521, test acc: 0.892626, test avg acc: 0.832971\n",
            "Train 65, loss: 1.649613, train acc: 0.877846, train avg acc: 0.812446\n",
            "Test 65, loss: 1.573815, test acc: 0.882901, test avg acc: 0.834948\n",
            "Train 66, loss: 1.647096, train acc: 0.877744, train avg acc: 0.814082\n",
            "Test 66, loss: 1.580443, test acc: 0.869935, test avg acc: 0.837849\n",
            "Train 67, loss: 1.645645, train acc: 0.879980, train avg acc: 0.819873\n",
            "Test 67, loss: 1.603682, test acc: 0.872366, test avg acc: 0.830355\n",
            "Train 68, loss: 1.637485, train acc: 0.884858, train avg acc: 0.825319\n",
            "Test 68, loss: 1.561778, test acc: 0.899514, test avg acc: 0.844285\n",
            "Current Best: 0.899514\n",
            "Train 69, loss: 1.644574, train acc: 0.880285, train avg acc: 0.818074\n",
            "Test 69, loss: 1.583513, test acc: 0.889789, test avg acc: 0.825895\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}