{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "mount_file_id": "1J0doI3og3LuC-mwJtOWhy8UH2YrJQTAh",
      "authorship_tag": "ABX9TyNEZBZ41ZuUXfORobOz5X27",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DIPTE/Deep-Geometry-Learning-on-Colab/blob/master/GS-Net_cloab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZl_N3zcBqHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive#colab 连接到托管代码执行程序\n",
        "drive.mount('/content/drive')#挂载云端硬盘"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aJnmE2chLvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/Untitled Folder/GS-Net')#切换工作目录"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GijYTyabeILO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/MingyeXu/GS-Net.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-i8X6ZjC1Mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls #查看工作目录\n",
        "!nvcc -V\n",
        "!apt install lshw -y\n",
        "!lshw -C display"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY81aqfDENLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3ab71acb-35de-473a-a33e-d3976f3804a0"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data.py  main.py   OP\t       __pycache__  util.py\n",
            "imgs\t model.py  pretrained  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eOWiw6bES-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc2yO3N7Fcx0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "0a18c976-bcd2-4940-92ad-12c031bc56f4"
      },
      "source": [
        "!nvcc -V"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDbksB_mG879",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sudo apt-get install build-essential python3-dev libssl-dev libffi-dev libxml2 libxml2-dev libxslt1-dev zlib1g-dev"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OhQyEfWHiJb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "c8094565-143e-4171-f546-f50f857bf162"
      },
      "source": [
        "!pip3 install torch==1.0.0 torchvision==0.2.1"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/60/66415660aa46b23b5e1b72bc762e816736ce8d7260213e22365af51e8f9c/torch-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (591.8MB)\n",
            "\u001b[K     |████████████████████████████████| 591.8MB 28kB/s \n",
            "\u001b[?25hCollecting torchvision==0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1) (1.12.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "  Found existing installation: torchvision 0.6.0+cu101\n",
            "    Uninstalling torchvision-0.6.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.0+cu101\n",
            "Successfully installed torch-1.0.0 torchvision-0.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_YKRh-0El_8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "4fdbaeb8-011c-46f4-fe93-52bddace4f04"
      },
      "source": [
        "!cd OP && python setup.py install && cd ../"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "writing pointnet2.egg-info/PKG-INFO\n",
            "writing dependency_links to pointnet2.egg-info/dependency_links.txt\n",
            "writing top-level names to pointnet2.egg-info/top_level.txt\n",
            "writing manifest file 'pointnet2.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_ext\n",
            "building 'pointnet2_cuda' extension\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c src/pointnet2_api.cpp -o build/temp.linux-x86_64-3.6/src/pointnet2_api.o -g -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=pointnet2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c src/ball_query.cpp -o build/temp.linux-x86_64-3.6/src/ball_query.o -g -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=pointnet2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c src/ball_query_gpu.cu -o build/temp.linux-x86_64-3.6/src/ball_query_gpu.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -O2 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=pointnet2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c src/group_points.cpp -o build/temp.linux-x86_64-3.6/src/group_points.o -g -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=pointnet2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c src/group_points_gpu.cu -o build/temp.linux-x86_64-3.6/src/group_points_gpu.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -O2 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=pointnet2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c src/interpolate.cpp -o build/temp.linux-x86_64-3.6/src/interpolate.o -g -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=pointnet2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c src/interpolate_gpu.cu -o build/temp.linux-x86_64-3.6/src/interpolate_gpu.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -O2 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=pointnet2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c src/sampling.cpp -o build/temp.linux-x86_64-3.6/src/sampling.o -g -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=pointnet2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "/usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/lib/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/lib/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c src/sampling_gpu.cu -o build/temp.linux-x86_64-3.6/src/sampling_gpu.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --compiler-options '-fPIC' -O2 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=pointnet2_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++11\n",
            "creating build/lib.linux-x86_64-3.6\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/src/pointnet2_api.o build/temp.linux-x86_64-3.6/src/ball_query.o build/temp.linux-x86_64-3.6/src/ball_query_gpu.o build/temp.linux-x86_64-3.6/src/group_points.o build/temp.linux-x86_64-3.6/src/group_points_gpu.o build/temp.linux-x86_64-3.6/src/interpolate.o build/temp.linux-x86_64-3.6/src/interpolate_gpu.o build/temp.linux-x86_64-3.6/src/sampling.o build/temp.linux-x86_64-3.6/src/sampling_gpu.o -L/usr/local/cuda/lib64 -lcudart -o build/lib.linux-x86_64-3.6/pointnet2_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.6/pointnet2_cuda.cpython-36m-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\n",
            "creating stub loader for pointnet2_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pointnet2_cuda.py to pointnet2_cuda.cpython-36.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pointnet2.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pointnet2.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pointnet2.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying pointnet2.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "__pycache__.pointnet2_cuda.cpython-36: module references __file__\n",
            "creating 'dist/pointnet2-0.0.0-py3.6-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing pointnet2-0.0.0-py3.6-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.6/dist-packages/pointnet2-0.0.0-py3.6-linux-x86_64.egg\n",
            "Extracting pointnet2-0.0.0-py3.6-linux-x86_64.egg to /usr/local/lib/python3.6/dist-packages\n",
            "Adding pointnet2 0.0.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.6/dist-packages/pointnet2-0.0.0-py3.6-linux-x86_64.egg\n",
            "Processing dependencies for pointnet2==0.0.0\n",
            "Finished processing dependencies for pointnet2==0.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mACYV7VoJInb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a597f5bc-270d-47df-d749-dbace7ac0bc0"
      },
      "source": [
        "!python main.py "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_size=64, dataset='modelnet40', dropout=0.5, emb_dims=1024, epochs=500, eval=False, exp_name='exp', k=20, lr=0.001, model='GSNET', model_path='', momentum=0.9, no_cuda=False, num_points=1024, seed=1, test_batch_size=32, use_sgd=False)\n",
            "Using GPU : 0 from 1 devices\n",
            "/content/drive/My Drive/Untitled Folder/GS-Net/data.py:29: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
            "  f = h5py.File(h5_name)\n",
            "GSNET(\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(13, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (conv5): Sequential(\n",
            "    (0): Conv1d(256, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
            "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): LeakyReLU(negative_slope=0.2)\n",
            "  )\n",
            "  (linear1): Linear(in_features=2048, out_features=512, bias=False)\n",
            "  (bn6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dp1): Dropout(p=0.5)\n",
            "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (bn7): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (dp2): Dropout(p=0.5)\n",
            "  (linear3): Linear(in_features=256, out_features=40, bias=True)\n",
            ")\n",
            "Let's use 1 GPUs!\n",
            "Use Adam\n",
            "Train 0, loss: 2.721393, train acc: 0.445159, train avg acc: 0.234789\n",
            "Test 0, loss: 2.364150, test acc: 0.518233, test avg acc: 0.386901\n",
            "Max Acc:0.518233\n",
            "Train 1, loss: 2.160762, train acc: 0.655433, train avg acc: 0.463036\n",
            "Test 1, loss: 2.133383, test acc: 0.653566, test avg acc: 0.520593\n",
            "Max Acc:0.653566\n",
            "Train 2, loss: 1.988462, train acc: 0.734886, train avg acc: 0.582417\n",
            "Test 2, loss: 1.887452, test acc: 0.781605, test avg acc: 0.662453\n",
            "Max Acc:0.781605\n",
            "Train 3, loss: 1.894739, train acc: 0.773999, train avg acc: 0.645247\n",
            "Test 3, loss: 1.765306, test acc: 0.819692, test avg acc: 0.713831\n",
            "Max Acc:0.819692\n",
            "Train 4, loss: 1.829130, train acc: 0.806373, train avg acc: 0.691660\n",
            "Test 4, loss: 1.735945, test acc: 0.844814, test avg acc: 0.752942\n",
            "Max Acc:0.844814\n",
            "Train 5, loss: 1.782380, train acc: 0.819240, train avg acc: 0.717806\n",
            "Test 5, loss: 1.733399, test acc: 0.831037, test avg acc: 0.752087\n",
            "Train 6, loss: 1.749186, train acc: 0.837010, train avg acc: 0.746482\n",
            "Test 6, loss: 1.704217, test acc: 0.845624, test avg acc: 0.773250\n",
            "Max Acc:0.845624\n",
            "Train 7, loss: 1.721841, train acc: 0.843035, train avg acc: 0.752380\n",
            "Test 7, loss: 1.680599, test acc: 0.851297, test avg acc: 0.779872\n",
            "Max Acc:0.851297\n",
            "Train 8, loss: 1.701369, train acc: 0.851920, train avg acc: 0.769127\n",
            "Test 8, loss: 1.633598, test acc: 0.865883, test avg acc: 0.793924\n",
            "Max Acc:0.865883\n",
            "Train 9, loss: 1.685463, train acc: 0.856413, train avg acc: 0.775440\n",
            "Test 9, loss: 1.616519, test acc: 0.877634, test avg acc: 0.813436\n",
            "Max Acc:0.877634\n",
            "Train 10, loss: 1.678706, train acc: 0.860498, train avg acc: 0.784281\n",
            "Test 10, loss: 1.638023, test acc: 0.862642, test avg acc: 0.798151\n",
            "Train 11, loss: 1.661062, train acc: 0.865605, train avg acc: 0.790823\n",
            "Test 11, loss: 1.609303, test acc: 0.869530, test avg acc: 0.790895\n",
            "Train 12, loss: 1.644300, train acc: 0.870507, train avg acc: 0.802841\n",
            "Test 12, loss: 1.600199, test acc: 0.877229, test avg acc: 0.824680\n",
            "Train 13, loss: 1.644668, train acc: 0.871119, train avg acc: 0.800900\n",
            "Test 13, loss: 1.661932, test acc: 0.879254, test avg acc: 0.810959\n",
            "Max Acc:0.879254\n",
            "Train 14, loss: 1.624687, train acc: 0.879596, train avg acc: 0.814741\n",
            "Test 14, loss: 1.609249, test acc: 0.856564, test avg acc: 0.791145\n",
            "Train 15, loss: 1.621673, train acc: 0.878676, train avg acc: 0.814405\n",
            "Test 15, loss: 1.618017, test acc: 0.865883, test avg acc: 0.817140\n",
            "Train 16, loss: 1.602927, train acc: 0.888685, train avg acc: 0.822452\n",
            "Test 16, loss: 1.590652, test acc: 0.884117, test avg acc: 0.831558\n",
            "Max Acc:0.884117\n",
            "Train 17, loss: 1.604558, train acc: 0.888889, train avg acc: 0.832342\n",
            "Test 17, loss: 1.551682, test acc: 0.895462, test avg acc: 0.839041\n",
            "Max Acc:0.895462\n",
            "Train 18, loss: 1.596981, train acc: 0.893995, train avg acc: 0.835743\n",
            "Test 18, loss: 1.562645, test acc: 0.882901, test avg acc: 0.822855\n",
            "Train 19, loss: 1.592665, train acc: 0.892361, train avg acc: 0.833803\n",
            "Test 19, loss: 1.562907, test acc: 0.885332, test avg acc: 0.826471\n",
            "Train 20, loss: 1.588249, train acc: 0.894199, train avg acc: 0.836074\n",
            "Test 20, loss: 1.552092, test acc: 0.895867, test avg acc: 0.840174\n",
            "Max Acc:0.895867\n",
            "Train 21, loss: 1.584327, train acc: 0.897365, train avg acc: 0.842598\n",
            "Test 21, loss: 1.541398, test acc: 0.898298, test avg acc: 0.853547\n",
            "Max Acc:0.898298\n",
            "Train 22, loss: 1.575640, train acc: 0.896650, train avg acc: 0.840157\n",
            "Test 22, loss: 1.557964, test acc: 0.890600, test avg acc: 0.841262\n",
            "Train 23, loss: 1.569596, train acc: 0.900020, train avg acc: 0.845934\n",
            "Test 23, loss: 1.565172, test acc: 0.887763, test avg acc: 0.838326\n",
            "Train 24, loss: 1.569058, train acc: 0.900633, train avg acc: 0.846658\n",
            "Test 24, loss: 1.562145, test acc: 0.888979, test avg acc: 0.847564\n",
            "Train 25, loss: 1.565889, train acc: 0.901757, train avg acc: 0.844762\n",
            "Test 25, loss: 1.548600, test acc: 0.892626, test avg acc: 0.858733\n",
            "Train 26, loss: 1.558316, train acc: 0.905025, train avg acc: 0.848287\n",
            "Test 26, loss: 1.547403, test acc: 0.897488, test avg acc: 0.857012\n",
            "Train 27, loss: 1.551690, train acc: 0.908395, train avg acc: 0.859515\n",
            "Test 27, loss: 1.536126, test acc: 0.899919, test avg acc: 0.857308\n",
            "Max Acc:0.899919\n",
            "Train 28, loss: 1.553252, train acc: 0.908599, train avg acc: 0.858400\n",
            "Test 28, loss: 1.511790, test acc: 0.907618, test avg acc: 0.853837\n",
            "Max Acc:0.907618\n",
            "Train 29, loss: 1.548709, train acc: 0.914216, train avg acc: 0.867291\n",
            "Test 29, loss: 1.553725, test acc: 0.891005, test avg acc: 0.843686\n",
            "Train 30, loss: 1.547068, train acc: 0.908905, train avg acc: 0.861872\n",
            "Test 30, loss: 1.552112, test acc: 0.908023, test avg acc: 0.861256\n",
            "Max Acc:0.908023\n",
            "Train 31, loss: 1.545753, train acc: 0.910029, train avg acc: 0.862705\n",
            "Test 31, loss: 1.577767, test acc: 0.900324, test avg acc: 0.859250\n",
            "Train 32, loss: 1.541142, train acc: 0.911867, train avg acc: 0.864932\n",
            "Test 32, loss: 1.513133, test acc: 0.907212, test avg acc: 0.867669\n",
            "Train 33, loss: 1.534058, train acc: 0.917586, train avg acc: 0.874792\n",
            "Test 33, loss: 1.518497, test acc: 0.901540, test avg acc: 0.862552\n",
            "Train 34, loss: 1.531401, train acc: 0.915543, train avg acc: 0.872182\n",
            "Test 34, loss: 1.514932, test acc: 0.902755, test avg acc: 0.851215\n",
            "Train 35, loss: 1.532561, train acc: 0.917177, train avg acc: 0.874767\n",
            "Test 35, loss: 1.508586, test acc: 0.906807, test avg acc: 0.863721\n",
            "Train 36, loss: 1.525529, train acc: 0.915543, train avg acc: 0.869516\n",
            "Test 36, loss: 1.519177, test acc: 0.908428, test avg acc: 0.864012\n",
            "Max Acc:0.908428\n",
            "Train 37, loss: 1.525792, train acc: 0.916156, train avg acc: 0.873168\n",
            "Test 37, loss: 1.591996, test acc: 0.891005, test avg acc: 0.831669\n",
            "Train 38, loss: 1.522343, train acc: 0.919322, train avg acc: 0.877215\n",
            "Test 38, loss: 1.525746, test acc: 0.905592, test avg acc: 0.854459\n",
            "Train 39, loss: 1.519887, train acc: 0.921058, train avg acc: 0.877428\n",
            "Test 39, loss: 1.493359, test acc: 0.910859, test avg acc: 0.861017\n",
            "Max Acc:0.910859\n",
            "Train 40, loss: 1.519446, train acc: 0.921467, train avg acc: 0.878276\n",
            "Test 40, loss: 1.505024, test acc: 0.903971, test avg acc: 0.846913\n",
            "Train 41, loss: 1.516126, train acc: 0.921262, train avg acc: 0.875856\n",
            "Test 41, loss: 1.507763, test acc: 0.902755, test avg acc: 0.861314\n",
            "Train 42, loss: 1.521666, train acc: 0.916258, train avg acc: 0.872114\n",
            "Test 42, loss: 1.513259, test acc: 0.910454, test avg acc: 0.866669\n",
            "Train 43, loss: 1.513255, train acc: 0.923611, train avg acc: 0.882935\n",
            "Test 43, loss: 1.512084, test acc: 0.910859, test avg acc: 0.873250\n",
            "Max Acc:0.910859\n",
            "Train 44, loss: 1.515918, train acc: 0.923917, train avg acc: 0.882521\n",
            "Test 44, loss: 1.507971, test acc: 0.905186, test avg acc: 0.868337\n",
            "Train 45, loss: 1.511296, train acc: 0.925143, train avg acc: 0.885624\n",
            "Test 45, loss: 1.510729, test acc: 0.906807, test avg acc: 0.871419\n",
            "Train 46, loss: 1.507818, train acc: 0.928105, train avg acc: 0.888923\n",
            "Test 46, loss: 1.497928, test acc: 0.914506, test avg acc: 0.867047\n",
            "Max Acc:0.914506\n",
            "Train 47, loss: 1.506215, train acc: 0.928002, train avg acc: 0.889173\n",
            "Test 47, loss: 1.514526, test acc: 0.895867, test avg acc: 0.854093\n",
            "Train 48, loss: 1.506938, train acc: 0.927798, train avg acc: 0.887496\n",
            "Test 48, loss: 1.512381, test acc: 0.907212, test avg acc: 0.861087\n",
            "Train 49, loss: 1.505863, train acc: 0.929534, train avg acc: 0.894136\n",
            "Test 49, loss: 1.508292, test acc: 0.905997, test avg acc: 0.861203\n",
            "Train 50, loss: 1.506310, train acc: 0.927798, train avg acc: 0.891024\n",
            "Test 50, loss: 1.508121, test acc: 0.908023, test avg acc: 0.857465\n",
            "Train 51, loss: 1.506918, train acc: 0.925041, train avg acc: 0.886159\n",
            "Test 51, loss: 1.528147, test acc: 0.903971, test avg acc: 0.856715\n",
            "Train 52, loss: 1.498325, train acc: 0.925347, train avg acc: 0.885913\n",
            "Test 52, loss: 1.478779, test acc: 0.916532, test avg acc: 0.868669\n",
            "Max Acc:0.916532\n",
            "Train 53, loss: 1.499939, train acc: 0.929636, train avg acc: 0.892939\n",
            "Test 53, loss: 1.493924, test acc: 0.914100, test avg acc: 0.865890\n",
            "Train 54, loss: 1.497722, train acc: 0.928615, train avg acc: 0.890860\n",
            "Test 54, loss: 1.488033, test acc: 0.919368, test avg acc: 0.879959\n",
            "Max Acc:0.919368\n",
            "Train 55, loss: 1.492740, train acc: 0.931781, train avg acc: 0.896102\n",
            "Test 55, loss: 1.491798, test acc: 0.908833, test avg acc: 0.861331\n",
            "Train 56, loss: 1.491574, train acc: 0.932496, train avg acc: 0.896986\n",
            "Test 56, loss: 1.522714, test acc: 0.899514, test avg acc: 0.864006\n",
            "Train 57, loss: 1.491654, train acc: 0.933109, train avg acc: 0.896853\n",
            "Test 57, loss: 1.496286, test acc: 0.902350, test avg acc: 0.857023\n",
            "Train 58, loss: 1.494337, train acc: 0.930351, train avg acc: 0.893298\n",
            "Test 58, loss: 1.490280, test acc: 0.910859, test avg acc: 0.868465\n",
            "Train 59, loss: 1.494482, train acc: 0.930862, train avg acc: 0.893039\n",
            "Test 59, loss: 1.494157, test acc: 0.904376, test avg acc: 0.865180\n",
            "Train 60, loss: 1.488433, train acc: 0.930453, train avg acc: 0.893099\n",
            "Test 60, loss: 1.493810, test acc: 0.906402, test avg acc: 0.862378\n",
            "Train 61, loss: 1.492682, train acc: 0.931781, train avg acc: 0.895788\n",
            "Test 61, loss: 1.522574, test acc: 0.905592, test avg acc: 0.862703\n",
            "Train 62, loss: 1.489957, train acc: 0.934641, train avg acc: 0.902011\n",
            "Test 62, loss: 1.497906, test acc: 0.913290, test avg acc: 0.866826\n",
            "Train 63, loss: 1.483711, train acc: 0.935253, train avg acc: 0.899730\n",
            "Test 63, loss: 1.504744, test acc: 0.901540, test avg acc: 0.860012\n",
            "Train 64, loss: 1.482852, train acc: 0.933313, train avg acc: 0.896008\n",
            "Test 64, loss: 1.516309, test acc: 0.902350, test avg acc: 0.852860\n",
            "Train 65, loss: 1.484556, train acc: 0.935764, train avg acc: 0.904657\n",
            "Test 65, loss: 1.498779, test acc: 0.902350, test avg acc: 0.864384\n",
            "Train 66, loss: 1.479208, train acc: 0.937092, train avg acc: 0.902455\n",
            "Test 66, loss: 1.506290, test acc: 0.895057, test avg acc: 0.856872\n",
            "Train 67, loss: 1.474561, train acc: 0.940053, train avg acc: 0.910808\n",
            "Test 67, loss: 1.500874, test acc: 0.907618, test avg acc: 0.862663\n",
            "Train 68, loss: 1.479977, train acc: 0.934641, train avg acc: 0.899042\n",
            "Test 68, loss: 1.506238, test acc: 0.905186, test avg acc: 0.854424\n",
            "Train 69, loss: 1.478347, train acc: 0.936785, train avg acc: 0.901462\n",
            "Test 69, loss: 1.524744, test acc: 0.906807, test avg acc: 0.870826\n",
            "Train 70, loss: 1.474698, train acc: 0.937704, train avg acc: 0.905033\n",
            "Test 70, loss: 1.492203, test acc: 0.906402, test avg acc: 0.871012\n",
            "Train 71, loss: 1.480146, train acc: 0.937500, train avg acc: 0.903388\n",
            "Test 71, loss: 1.515336, test acc: 0.909238, test avg acc: 0.872500\n",
            "Train 72, loss: 1.471457, train acc: 0.939236, train avg acc: 0.906096\n",
            "Test 72, loss: 1.494537, test acc: 0.907212, test avg acc: 0.871047\n",
            "Train 73, loss: 1.474125, train acc: 0.939542, train avg acc: 0.904145\n",
            "Test 73, loss: 1.479530, test acc: 0.915316, test avg acc: 0.871517\n",
            "Train 74, loss: 1.479681, train acc: 0.936479, train avg acc: 0.903387\n",
            "Test 74, loss: 1.490528, test acc: 0.915316, test avg acc: 0.873628\n",
            "Train 75, loss: 1.471338, train acc: 0.940359, train avg acc: 0.907299\n",
            "Test 75, loss: 1.485749, test acc: 0.910454, test avg acc: 0.861535\n",
            "Train 76, loss: 1.475805, train acc: 0.936377, train avg acc: 0.902333\n",
            "Test 76, loss: 1.505483, test acc: 0.910049, test avg acc: 0.874041\n",
            "Train 77, loss: 1.474928, train acc: 0.938317, train avg acc: 0.905238\n",
            "Test 77, loss: 1.474850, test acc: 0.915721, test avg acc: 0.873802\n",
            "Train 78, loss: 1.469231, train acc: 0.943832, train avg acc: 0.912956\n",
            "Test 78, loss: 1.480410, test acc: 0.914100, test avg acc: 0.879552\n",
            "Train 79, loss: 1.477506, train acc: 0.938419, train avg acc: 0.905844\n",
            "Test 79, loss: 1.485748, test acc: 0.912885, test avg acc: 0.866512\n",
            "Train 80, loss: 1.470042, train acc: 0.940257, train avg acc: 0.907731\n",
            "Test 80, loss: 1.500717, test acc: 0.909238, test avg acc: 0.875279\n",
            "Train 81, loss: 1.468102, train acc: 0.940564, train avg acc: 0.911499\n",
            "Test 81, loss: 1.470175, test acc: 0.912480, test avg acc: 0.868384\n",
            "Train 82, loss: 1.472014, train acc: 0.939849, train avg acc: 0.909040\n",
            "Test 82, loss: 1.497731, test acc: 0.904781, test avg acc: 0.877285\n",
            "Train 83, loss: 1.469936, train acc: 0.940359, train avg acc: 0.910086\n",
            "Test 83, loss: 1.511624, test acc: 0.910049, test avg acc: 0.870500\n",
            "Train 84, loss: 1.464861, train acc: 0.943219, train avg acc: 0.913666\n",
            "Test 84, loss: 1.481455, test acc: 0.912885, test avg acc: 0.869012\n",
            "Train 85, loss: 1.465326, train acc: 0.942504, train avg acc: 0.913323\n",
            "Test 85, loss: 1.526452, test acc: 0.906807, test avg acc: 0.866128\n",
            "Train 86, loss: 1.461811, train acc: 0.943525, train avg acc: 0.911291\n",
            "Test 86, loss: 1.485371, test acc: 0.912885, test avg acc: 0.879424\n",
            "Train 87, loss: 1.469048, train acc: 0.940053, train avg acc: 0.909056\n",
            "Test 87, loss: 1.488007, test acc: 0.906807, test avg acc: 0.865000\n",
            "Train 88, loss: 1.462829, train acc: 0.943934, train avg acc: 0.914121\n",
            "Test 88, loss: 1.476760, test acc: 0.911264, test avg acc: 0.873680\n",
            "Train 89, loss: 1.462185, train acc: 0.943832, train avg acc: 0.913382\n",
            "Test 89, loss: 1.505698, test acc: 0.903971, test avg acc: 0.860721\n",
            "Train 90, loss: 1.463861, train acc: 0.941687, train avg acc: 0.910858\n",
            "Test 90, loss: 1.489669, test acc: 0.916532, test avg acc: 0.864326\n",
            "Train 91, loss: 1.459909, train acc: 0.943219, train avg acc: 0.914223\n",
            "Test 91, loss: 1.494154, test acc: 0.905997, test avg acc: 0.867924\n",
            "Train 92, loss: 1.461055, train acc: 0.946385, train avg acc: 0.917960\n",
            "Test 92, loss: 1.474393, test acc: 0.919368, test avg acc: 0.890012\n",
            "Max Acc:0.919368\n",
            "Train 93, loss: 1.462301, train acc: 0.944036, train avg acc: 0.915878\n",
            "Test 93, loss: 1.497248, test acc: 0.905997, test avg acc: 0.877930\n",
            "Train 94, loss: 1.460898, train acc: 0.946487, train avg acc: 0.918050\n",
            "Test 94, loss: 1.490223, test acc: 0.905592, test avg acc: 0.872587\n",
            "Train 95, loss: 1.459017, train acc: 0.946895, train avg acc: 0.919034\n",
            "Test 95, loss: 1.485949, test acc: 0.907618, test avg acc: 0.870837\n",
            "Train 96, loss: 1.458923, train acc: 0.944751, train avg acc: 0.916053\n",
            "Test 96, loss: 1.500197, test acc: 0.896272, test avg acc: 0.859244\n",
            "Train 97, loss: 1.453456, train acc: 0.946998, train avg acc: 0.920027\n",
            "Test 97, loss: 1.475352, test acc: 0.912480, test avg acc: 0.875762\n",
            "Train 98, loss: 1.457070, train acc: 0.944444, train avg acc: 0.913456\n",
            "Test 98, loss: 1.484522, test acc: 0.916937, test avg acc: 0.881000\n",
            "Train 99, loss: 1.459286, train acc: 0.944649, train avg acc: 0.913258\n",
            "Test 99, loss: 1.493638, test acc: 0.908428, test avg acc: 0.870959\n",
            "Train 100, loss: 1.454266, train acc: 0.947917, train avg acc: 0.918974\n",
            "Test 100, loss: 1.483830, test acc: 0.903160, test avg acc: 0.863302\n",
            "Train 101, loss: 1.456551, train acc: 0.942810, train avg acc: 0.910770\n",
            "Test 101, loss: 1.464645, test acc: 0.913695, test avg acc: 0.883628\n",
            "Train 102, loss: 1.450988, train acc: 0.948938, train avg acc: 0.922417\n",
            "Test 102, loss: 1.501721, test acc: 0.903566, test avg acc: 0.871302\n",
            "Train 103, loss: 1.456799, train acc: 0.944444, train avg acc: 0.914380\n",
            "Test 103, loss: 1.475429, test acc: 0.910859, test avg acc: 0.877093\n",
            "Train 104, loss: 1.455118, train acc: 0.948325, train avg acc: 0.924407\n",
            "Test 104, loss: 1.491714, test acc: 0.902755, test avg acc: 0.871337\n",
            "Train 105, loss: 1.452199, train acc: 0.946895, train avg acc: 0.919781\n",
            "Test 105, loss: 1.505075, test acc: 0.910049, test avg acc: 0.870674\n",
            "Train 106, loss: 1.450940, train acc: 0.948121, train avg acc: 0.922866\n",
            "Test 106, loss: 1.476632, test acc: 0.918558, test avg acc: 0.876837\n",
            "Train 107, loss: 1.454239, train acc: 0.946181, train avg acc: 0.916543\n",
            "Test 107, loss: 1.491338, test acc: 0.913695, test avg acc: 0.876756\n",
            "Train 108, loss: 1.455170, train acc: 0.945670, train avg acc: 0.917912\n",
            "Test 108, loss: 1.515190, test acc: 0.900324, test avg acc: 0.880041\n",
            "Train 109, loss: 1.452324, train acc: 0.947406, train avg acc: 0.920316\n",
            "Test 109, loss: 1.478005, test acc: 0.905997, test avg acc: 0.870302\n",
            "Train 110, loss: 1.449402, train acc: 0.949142, train avg acc: 0.923668\n",
            "Test 110, loss: 1.509935, test acc: 0.909643, test avg acc: 0.864227\n",
            "Train 111, loss: 1.447988, train acc: 0.949959, train avg acc: 0.926768\n",
            "Test 111, loss: 1.473373, test acc: 0.920989, test avg acc: 0.880291\n",
            "Max Acc:0.920989\n",
            "Train 112, loss: 1.450020, train acc: 0.949040, train avg acc: 0.924904\n",
            "Test 112, loss: 1.520431, test acc: 0.910454, test avg acc: 0.878831\n",
            "Train 113, loss: 1.444791, train acc: 0.951593, train avg acc: 0.926546\n",
            "Test 113, loss: 1.470300, test acc: 0.916937, test avg acc: 0.881134\n",
            "Train 114, loss: 1.453688, train acc: 0.943627, train avg acc: 0.911910\n",
            "Test 114, loss: 1.494887, test acc: 0.911669, test avg acc: 0.887924\n",
            "Train 115, loss: 1.444109, train acc: 0.950776, train avg acc: 0.925391\n",
            "Test 115, loss: 1.464451, test acc: 0.911264, test avg acc: 0.876971\n",
            "Train 116, loss: 1.442249, train acc: 0.952717, train avg acc: 0.929963\n",
            "Test 116, loss: 1.493299, test acc: 0.900729, test avg acc: 0.863849\n",
            "Train 117, loss: 1.450232, train acc: 0.945466, train avg acc: 0.917505\n",
            "Test 117, loss: 1.481276, test acc: 0.909643, test avg acc: 0.878628\n",
            "Train 118, loss: 1.447825, train acc: 0.946998, train avg acc: 0.919596\n",
            "Test 118, loss: 1.476970, test acc: 0.920989, test avg acc: 0.889674\n",
            "Max Acc:0.920989\n",
            "Train 119, loss: 1.442712, train acc: 0.948427, train avg acc: 0.921725\n",
            "Test 119, loss: 1.476013, test acc: 0.911669, test avg acc: 0.880105\n",
            "Train 120, loss: 1.444873, train acc: 0.950776, train avg acc: 0.926089\n",
            "Test 120, loss: 1.517010, test acc: 0.908428, test avg acc: 0.879378\n",
            "Train 121, loss: 1.440432, train acc: 0.952921, train avg acc: 0.930031\n",
            "Test 121, loss: 1.473941, test acc: 0.911264, test avg acc: 0.873390\n",
            "Train 122, loss: 1.440838, train acc: 0.954657, train avg acc: 0.932310\n",
            "Test 122, loss: 1.492267, test acc: 0.915316, test avg acc: 0.872715\n",
            "Train 123, loss: 1.447487, train acc: 0.948529, train avg acc: 0.920985\n",
            "Test 123, loss: 1.489585, test acc: 0.914100, test avg acc: 0.881506\n",
            "Train 124, loss: 1.446134, train acc: 0.949551, train avg acc: 0.921630\n",
            "Test 124, loss: 1.481745, test acc: 0.911264, test avg acc: 0.873791\n",
            "Train 125, loss: 1.435680, train acc: 0.953023, train avg acc: 0.928448\n",
            "Test 125, loss: 1.469418, test acc: 0.916126, test avg acc: 0.878174\n",
            "Train 126, loss: 1.437964, train acc: 0.955167, train avg acc: 0.933052\n",
            "Test 126, loss: 1.555962, test acc: 0.898298, test avg acc: 0.849331\n",
            "Train 127, loss: 1.444317, train acc: 0.949346, train avg acc: 0.923548\n",
            "Test 127, loss: 1.470635, test acc: 0.915721, test avg acc: 0.884262\n",
            "Train 128, loss: 1.441234, train acc: 0.951491, train avg acc: 0.927549\n",
            "Test 128, loss: 1.485273, test acc: 0.908428, test avg acc: 0.878465\n",
            "Train 129, loss: 1.438941, train acc: 0.951491, train avg acc: 0.925405\n",
            "Test 129, loss: 1.477982, test acc: 0.917747, test avg acc: 0.878581\n",
            "Train 130, loss: 1.441448, train acc: 0.950061, train avg acc: 0.926650\n",
            "Test 130, loss: 1.465512, test acc: 0.917747, test avg acc: 0.879052\n",
            "Train 131, loss: 1.440264, train acc: 0.953329, train avg acc: 0.928001\n",
            "Test 131, loss: 1.479446, test acc: 0.912885, test avg acc: 0.883331\n",
            "Train 132, loss: 1.440150, train acc: 0.951491, train avg acc: 0.927264\n",
            "Test 132, loss: 1.471115, test acc: 0.911669, test avg acc: 0.872965\n",
            "Train 133, loss: 1.435052, train acc: 0.954861, train avg acc: 0.934052\n",
            "Test 133, loss: 1.464956, test acc: 0.917342, test avg acc: 0.871337\n",
            "Train 134, loss: 1.434995, train acc: 0.954350, train avg acc: 0.931076\n",
            "Test 134, loss: 1.472026, test acc: 0.913290, test avg acc: 0.874552\n",
            "Train 135, loss: 1.433934, train acc: 0.950878, train avg acc: 0.927073\n",
            "Test 135, loss: 1.465127, test acc: 0.921394, test avg acc: 0.887076\n",
            "Max Acc:0.921394\n",
            "Train 136, loss: 1.438737, train acc: 0.951900, train avg acc: 0.926134\n",
            "Test 136, loss: 1.478194, test acc: 0.912075, test avg acc: 0.876041\n",
            "Train 137, loss: 1.432315, train acc: 0.955984, train avg acc: 0.932720\n",
            "Test 137, loss: 1.468140, test acc: 0.911669, test avg acc: 0.873407\n",
            "Train 138, loss: 1.433167, train acc: 0.954963, train avg acc: 0.929333\n",
            "Test 138, loss: 1.495608, test acc: 0.909643, test avg acc: 0.862500\n",
            "Train 139, loss: 1.433817, train acc: 0.956699, train avg acc: 0.934935\n",
            "Test 139, loss: 1.470218, test acc: 0.908428, test avg acc: 0.877291\n",
            "Train 140, loss: 1.435367, train acc: 0.954248, train avg acc: 0.930796\n",
            "Test 140, loss: 1.483984, test acc: 0.906402, test avg acc: 0.864099\n",
            "Train 141, loss: 1.432285, train acc: 0.954759, train avg acc: 0.932357\n",
            "Test 141, loss: 1.475862, test acc: 0.912480, test avg acc: 0.892250\n",
            "Train 142, loss: 1.431965, train acc: 0.955065, train avg acc: 0.931527\n",
            "Test 142, loss: 1.468821, test acc: 0.917747, test avg acc: 0.884180\n",
            "Train 143, loss: 1.433643, train acc: 0.957006, train avg acc: 0.935603\n",
            "Test 143, loss: 1.484332, test acc: 0.910859, test avg acc: 0.876297\n",
            "Train 144, loss: 1.430820, train acc: 0.954861, train avg acc: 0.931074\n",
            "Test 144, loss: 1.460873, test acc: 0.915316, test avg acc: 0.873453\n",
            "Train 145, loss: 1.429089, train acc: 0.957210, train avg acc: 0.936415\n",
            "Test 145, loss: 1.483202, test acc: 0.909643, test avg acc: 0.870384\n",
            "Train 146, loss: 1.431561, train acc: 0.955780, train avg acc: 0.935151\n",
            "Test 146, loss: 1.475636, test acc: 0.914506, test avg acc: 0.872297\n",
            "Train 147, loss: 1.425327, train acc: 0.956189, train avg acc: 0.933128\n",
            "Test 147, loss: 1.470198, test acc: 0.912480, test avg acc: 0.876081\n",
            "Train 148, loss: 1.430960, train acc: 0.956597, train avg acc: 0.934588\n",
            "Test 148, loss: 1.475088, test acc: 0.919773, test avg acc: 0.878593\n",
            "Train 149, loss: 1.431270, train acc: 0.955474, train avg acc: 0.931625\n",
            "Test 149, loss: 1.471834, test acc: 0.910049, test avg acc: 0.880506\n",
            "Train 150, loss: 1.424591, train acc: 0.959150, train avg acc: 0.937056\n",
            "Test 150, loss: 1.468064, test acc: 0.911669, test avg acc: 0.875273\n",
            "Train 151, loss: 1.430741, train acc: 0.956189, train avg acc: 0.935491\n",
            "Test 151, loss: 1.484860, test acc: 0.906807, test avg acc: 0.866459\n",
            "Train 152, loss: 1.432661, train acc: 0.954350, train avg acc: 0.929893\n",
            "Test 152, loss: 1.477606, test acc: 0.919368, test avg acc: 0.885215\n",
            "Train 153, loss: 1.428573, train acc: 0.955984, train avg acc: 0.934219\n",
            "Test 153, loss: 1.479793, test acc: 0.910859, test avg acc: 0.861959\n",
            "Train 154, loss: 1.424758, train acc: 0.956291, train avg acc: 0.932658\n",
            "Test 154, loss: 1.465213, test acc: 0.910454, test avg acc: 0.878413\n",
            "Train 155, loss: 1.425975, train acc: 0.954861, train avg acc: 0.929436\n",
            "Test 155, loss: 1.471482, test acc: 0.918152, test avg acc: 0.878349\n",
            "Train 156, loss: 1.425452, train acc: 0.958946, train avg acc: 0.936492\n",
            "Test 156, loss: 1.475701, test acc: 0.916532, test avg acc: 0.890297\n",
            "Train 157, loss: 1.427190, train acc: 0.957006, train avg acc: 0.935459\n",
            "Test 157, loss: 1.475783, test acc: 0.912480, test avg acc: 0.869052\n",
            "Train 158, loss: 1.424818, train acc: 0.956597, train avg acc: 0.937079\n",
            "Test 158, loss: 1.487592, test acc: 0.906402, test avg acc: 0.886203\n",
            "Train 159, loss: 1.423530, train acc: 0.958538, train avg acc: 0.938289\n",
            "Test 159, loss: 1.490287, test acc: 0.914911, test avg acc: 0.877849\n",
            "Train 160, loss: 1.425445, train acc: 0.957721, train avg acc: 0.937174\n",
            "Test 160, loss: 1.490943, test acc: 0.900729, test avg acc: 0.862663\n",
            "Train 161, loss: 1.419965, train acc: 0.960989, train avg acc: 0.940060\n",
            "Test 161, loss: 1.475143, test acc: 0.917747, test avg acc: 0.879093\n",
            "Train 162, loss: 1.422189, train acc: 0.961908, train avg acc: 0.941046\n",
            "Test 162, loss: 1.471499, test acc: 0.909643, test avg acc: 0.874802\n",
            "Train 163, loss: 1.420990, train acc: 0.958640, train avg acc: 0.939109\n",
            "Test 163, loss: 1.499655, test acc: 0.901540, test avg acc: 0.869302\n",
            "Train 164, loss: 1.422770, train acc: 0.958946, train avg acc: 0.935906\n",
            "Test 164, loss: 1.459228, test acc: 0.914506, test avg acc: 0.871209\n",
            "Train 165, loss: 1.421572, train acc: 0.958129, train avg acc: 0.937812\n",
            "Test 165, loss: 1.473108, test acc: 0.910454, test avg acc: 0.869709\n",
            "Train 166, loss: 1.421429, train acc: 0.959763, train avg acc: 0.937851\n",
            "Test 166, loss: 1.469557, test acc: 0.915316, test avg acc: 0.871366\n",
            "Train 167, loss: 1.421379, train acc: 0.958742, train avg acc: 0.938010\n",
            "Test 167, loss: 1.495483, test acc: 0.905997, test avg acc: 0.877064\n",
            "Train 168, loss: 1.421302, train acc: 0.960172, train avg acc: 0.938704\n",
            "Test 168, loss: 1.523411, test acc: 0.906402, test avg acc: 0.871791\n",
            "Train 169, loss: 1.417883, train acc: 0.961806, train avg acc: 0.944147\n",
            "Test 169, loss: 1.457606, test acc: 0.916126, test avg acc: 0.880494\n",
            "Train 170, loss: 1.414142, train acc: 0.962725, train avg acc: 0.943808\n",
            "Test 170, loss: 1.463447, test acc: 0.916532, test avg acc: 0.875506\n",
            "Train 171, loss: 1.418600, train acc: 0.959763, train avg acc: 0.939898\n",
            "Test 171, loss: 1.469973, test acc: 0.914100, test avg acc: 0.878343\n",
            "Train 172, loss: 1.421630, train acc: 0.959457, train avg acc: 0.938722\n",
            "Test 172, loss: 1.493067, test acc: 0.913695, test avg acc: 0.882709\n",
            "Train 173, loss: 1.421923, train acc: 0.960274, train avg acc: 0.938929\n",
            "Test 173, loss: 1.456063, test acc: 0.919773, test avg acc: 0.886343\n",
            "Train 174, loss: 1.417656, train acc: 0.961601, train avg acc: 0.941454\n",
            "Test 174, loss: 1.486323, test acc: 0.914506, test avg acc: 0.869959\n",
            "Train 175, loss: 1.419103, train acc: 0.961193, train avg acc: 0.940844\n",
            "Test 175, loss: 1.470597, test acc: 0.913695, test avg acc: 0.885413\n",
            "Train 176, loss: 1.415765, train acc: 0.962929, train avg acc: 0.944266\n",
            "Test 176, loss: 1.454880, test acc: 0.917342, test avg acc: 0.878419\n",
            "Train 177, loss: 1.415638, train acc: 0.959865, train avg acc: 0.938377\n",
            "Test 177, loss: 1.462934, test acc: 0.916937, test avg acc: 0.891076\n",
            "Train 178, loss: 1.413375, train acc: 0.962112, train avg acc: 0.941938\n",
            "Test 178, loss: 1.459148, test acc: 0.921799, test avg acc: 0.886506\n",
            "Max Acc:0.921799\n",
            "Train 179, loss: 1.415832, train acc: 0.962112, train avg acc: 0.943006\n",
            "Test 179, loss: 1.466969, test acc: 0.911669, test avg acc: 0.882535\n",
            "Train 180, loss: 1.414012, train acc: 0.963440, train avg acc: 0.943868\n",
            "Test 180, loss: 1.457032, test acc: 0.918558, test avg acc: 0.881628\n",
            "Train 181, loss: 1.412275, train acc: 0.961295, train avg acc: 0.940807\n",
            "Test 181, loss: 1.482284, test acc: 0.918963, test avg acc: 0.888866\n",
            "Train 182, loss: 1.415804, train acc: 0.960989, train avg acc: 0.939946\n",
            "Test 182, loss: 1.452883, test acc: 0.923420, test avg acc: 0.880250\n",
            "Max Acc:0.923420\n",
            "Train 183, loss: 1.411893, train acc: 0.965482, train avg acc: 0.946336\n",
            "Test 183, loss: 1.493565, test acc: 0.901540, test avg acc: 0.877744\n",
            "Train 184, loss: 1.412491, train acc: 0.963542, train avg acc: 0.945182\n",
            "Test 184, loss: 1.465099, test acc: 0.916126, test avg acc: 0.879413\n",
            "Train 185, loss: 1.415902, train acc: 0.962929, train avg acc: 0.944359\n",
            "Test 185, loss: 1.464962, test acc: 0.918558, test avg acc: 0.885041\n",
            "Train 186, loss: 1.409151, train acc: 0.964052, train avg acc: 0.946735\n",
            "Test 186, loss: 1.470573, test acc: 0.911264, test avg acc: 0.879703\n",
            "Train 187, loss: 1.414274, train acc: 0.961703, train avg acc: 0.945122\n",
            "Test 187, loss: 1.468193, test acc: 0.906402, test avg acc: 0.876174\n",
            "Train 188, loss: 1.411956, train acc: 0.963644, train avg acc: 0.946704\n",
            "Test 188, loss: 1.456489, test acc: 0.920178, test avg acc: 0.881674\n",
            "Train 189, loss: 1.410508, train acc: 0.963133, train avg acc: 0.943899\n",
            "Test 189, loss: 1.460374, test acc: 0.912885, test avg acc: 0.873419\n",
            "Train 190, loss: 1.411018, train acc: 0.963440, train avg acc: 0.942651\n",
            "Test 190, loss: 1.482985, test acc: 0.899514, test avg acc: 0.867419\n",
            "Train 191, loss: 1.409369, train acc: 0.963440, train avg acc: 0.945207\n",
            "Test 191, loss: 1.467468, test acc: 0.911264, test avg acc: 0.878628\n",
            "Train 192, loss: 1.409223, train acc: 0.964359, train avg acc: 0.944331\n",
            "Test 192, loss: 1.479467, test acc: 0.909643, test avg acc: 0.875872\n",
            "Train 193, loss: 1.405781, train acc: 0.966095, train avg acc: 0.950130\n",
            "Test 193, loss: 1.474104, test acc: 0.919368, test avg acc: 0.875500\n",
            "Train 194, loss: 1.410471, train acc: 0.964665, train avg acc: 0.947938\n",
            "Test 194, loss: 1.472524, test acc: 0.912075, test avg acc: 0.884593\n",
            "Train 195, loss: 1.406720, train acc: 0.966810, train avg acc: 0.949151\n",
            "Test 195, loss: 1.467518, test acc: 0.919368, test avg acc: 0.889791\n",
            "Train 196, loss: 1.405025, train acc: 0.964154, train avg acc: 0.946972\n",
            "Test 196, loss: 1.464015, test acc: 0.913695, test avg acc: 0.881163\n",
            "Train 197, loss: 1.405512, train acc: 0.966708, train avg acc: 0.950857\n",
            "Test 197, loss: 1.466308, test acc: 0.907618, test avg acc: 0.868006\n",
            "Train 198, loss: 1.407289, train acc: 0.965891, train avg acc: 0.948759\n",
            "Test 198, loss: 1.472266, test acc: 0.906402, test avg acc: 0.875750\n",
            "Train 199, loss: 1.404652, train acc: 0.965584, train avg acc: 0.947378\n",
            "Test 199, loss: 1.468326, test acc: 0.913290, test avg acc: 0.890994\n",
            "Train 200, loss: 1.404728, train acc: 0.966197, train avg acc: 0.950417\n",
            "Test 200, loss: 1.465835, test acc: 0.912885, test avg acc: 0.877465\n",
            "Train 201, loss: 1.405377, train acc: 0.966503, train avg acc: 0.950553\n",
            "Test 201, loss: 1.467515, test acc: 0.910049, test avg acc: 0.878081\n",
            "Train 202, loss: 1.408447, train acc: 0.963950, train avg acc: 0.946079\n",
            "Test 202, loss: 1.461674, test acc: 0.914100, test avg acc: 0.884703\n",
            "Train 203, loss: 1.402305, train acc: 0.965074, train avg acc: 0.944755\n",
            "Test 203, loss: 1.453384, test acc: 0.920989, test avg acc: 0.883500\n",
            "Train 204, loss: 1.403857, train acc: 0.964052, train avg acc: 0.946551\n",
            "Test 204, loss: 1.463665, test acc: 0.914911, test avg acc: 0.876041\n",
            "Train 205, loss: 1.403470, train acc: 0.966708, train avg acc: 0.950535\n",
            "Test 205, loss: 1.453787, test acc: 0.915316, test avg acc: 0.878674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0GGCbxmKvWf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "20e085b6-26da-4ae2-93cf-577a6769ad27"
      },
      "source": [
        "!cd /content/drive/My Drive/Untitled Folder/GS-Net/data/ && !wget https://shapenet.cs.stanford.edu/media/modelnet40_ply_hdf5_2048.zip && unzip modelnet40_ply_hdf5_2048.zip && rm modelnet40_ply_hdf5_2048.zip#下载数据集"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: cd: too many arguments\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}